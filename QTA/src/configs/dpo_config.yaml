
# DPO训练配置
dpo:
  # 基础训练参数
  training:
    # 输出目录
    output_dir: "./dpo_outputs"
    # DPO beta参数（控制KL惩罚强度）
    beta: 0.1
    # 训练轮数
    num_train_epochs: 3
    # 每个设备的批次大小
    per_device_train_batch_size: 2
    # 梯度累积步数
    gradient_accumulation_steps: 8
    # 学习率
    learning_rate: 1e-5
    # 是否使用梯度检查点
    gradient_checkpointing: true
    # 评估步数
    eval_steps: 100
    # 保存步数
    save_steps: 100
    # 日志记录步数
    logging_steps: 10
    # 最大保存检查点数量
    save_total_limit: 3

  # 序列长度配置
  sequence_length:
    # 最大提示长度
    max_prompt_length: 512
    # 最大序列总长度
    max_length: 1024
    # 是否填充到最大长度
    pad_to_max_length: true

  # 数据配置
  data:
    # 训练数据路径
    train_file: "data/processed/dpo_train.json"
    # 验证数据路径
    validation_file: "data/processed/dpo_validation.json"
    # 数据格式
    format: "json"
    # 提示词列名
    prompt_column: "prompt"
    # 选中回复列名
    chosen_column: "chosen"
    # 拒绝回复列名
    rejected_column: "rejected"

  # 优化器配置
  optimizer:
    # 优化器类型
    name: "adamw_torch"
    # 权重衰减
    weight_decay: 0.01
    # 梯度裁剪
    max_grad_norm: 1.0
    # 学习率调度器
    lr_scheduler_type: "cosine"
    # 预热步数比例
    warmup_ratio: 0.1

  # 评估配置
  evaluation:
    # 评估指标
    metrics:
      - "accuracy"
      - "preference_score"
    # 评估间隔（步数）
    eval_steps: 100
    # 是否在训练结束时评估
    evaluate_at_end: true

  # 日志配置
  logging:
    # wandb配置（如果使用）
    wandb:
      # 是否启用
      enabled: false
      # 项目名称
      project: "travel_agent_dpo"
      # 实验名称
      name: null

